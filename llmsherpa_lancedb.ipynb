{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with llmsherpa (nlm-ingestor), lancedb and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple and effective chunking was made possible with [llmsherpa](https://github.com/nlmatics/llmsherpa). \n",
    "- [nlm-ingester](https://github.com/nlmatics/nlm-ingestor) allowed self-hosted server for llmsherpa with docker.\n",
    "- OpenAI for embedding and chat completion (can be swapped to other alternatives).\n",
    "- This notebook provides an example for loading chunks created from llmsherpa to lancedb. Also uses in-built hybrid search from lancedb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull and run the docker image of nlm-ingester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull ghcr.io/nlmatics/nlm-ingestor:latest\n",
    "!docker run -p 5010:5001 ghcr.io/nlmatics/nlm-ingestor:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required libraries or use requirements.txt to install in a venv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llmsherpa\n",
      "  Using cached llmsherpa-0.1.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting lancedb\n",
      "  Using cached lancedb-0.6.13-cp38-abi3-macosx_10_15_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.30.1-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting urllib3 (from llmsherpa)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting deprecation (from lancedb)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pylance==0.10.12 (from lancedb)\n",
      "  Using cached pylance-0.10.12-cp38-abi3-macosx_10_15_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting ratelimiter~=1.0 (from lancedb)\n",
      "  Using cached ratelimiter-1.2.0.post0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting requests>=2.31.0 (from lancedb)\n",
      "  Downloading requests-2.32.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting retry>=0.9.2 (from lancedb)\n",
      "  Using cached retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting tqdm>=4.27.0 (from lancedb)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pydantic>=1.10 (from lancedb)\n",
      "  Using cached pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "Collecting attrs>=21.3.0 (from lancedb)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting semver (from lancedb)\n",
      "  Using cached semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting cachetools (from lancedb)\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting overrides>=0.7 (from lancedb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyarrow<15.0.1,>=12 (from pylance==0.10.12->lancedb)\n",
      "  Using cached pyarrow-15.0.0-cp311-cp311-macosx_10_15_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.22 (from pylance==0.10.12->lancedb)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nancyzzz/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.3.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/nancyzzz/Library/Python/3.11/lib/python/site-packages (from openai) (4.11.0)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10->lancedb)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic>=1.10->lancedb)\n",
      "  Using cached pydantic_core-2.18.2-cp311-cp311-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nancyzzz/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.31.0->lancedb)\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /Users/nancyzzz/Library/Python/3.11/lib/python/site-packages (from retry>=0.9.2->lancedb) (5.1.1)\n",
      "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb)\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: packaging in /Users/nancyzzz/Library/Python/3.11/lib/python/site-packages (from deprecation->lancedb) (24.0)\n",
      "Using cached llmsherpa-0.1.4-py3-none-any.whl (13 kB)\n",
      "Using cached lancedb-0.6.13-cp38-abi3-macosx_10_15_x86_64.whl (18.0 MB)\n",
      "Using cached pylance-0.10.12-cp38-abi3-macosx_10_15_x86_64.whl (21.3 MB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-macosx_10_9_x86_64.whl (12.6 MB)\n",
      "Downloading openai-1.30.1-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.6/320.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached anyio-4.3.0-py3-none-any.whl (85 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "Using cached pydantic_core-2.18.2-cp311-cp311-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
      "Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_10_9_x86_64.whl (121 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Using cached pyarrow-15.0.0-cp311-cp311-macosx_10_15_x86_64.whl (27.2 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: ratelimiter, pytz, urllib3, tzdata, tqdm, sniffio, semver, pydantic-core, py, overrides, numpy, idna, h11, distro, deprecation, charset-normalizer, cachetools, attrs, annotated-types, retry, requests, pydantic, pyarrow, pandas, llmsherpa, httpcore, anyio, pylance, httpx, openai, lancedb\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.3.0 attrs-23.2.0 cachetools-5.3.3 charset-normalizer-3.3.2 deprecation-2.1.0 distro-1.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.7 lancedb-0.6.13 llmsherpa-0.1.4 numpy-1.26.4 openai-1.30.1 overrides-7.7.0 pandas-2.2.2 py-1.11.0 pyarrow-15.0.0 pydantic-2.7.1 pydantic-core-2.18.2 pylance-0.10.12 pytz-2024.1 ratelimiter-1.2.0.post0 requests-2.32.2 retry-0.9.2 semver-3.0.2 sniffio-1.3.1 tqdm-4.66.4 tzdata-2024.1 urllib3-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install llmsherpa lancedb pandas openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tantivy\n",
      "  Using cached tantivy-0.22.0-cp311-cp311-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl.metadata (1.2 kB)\n",
      "Using cached tantivy-0.22.0-cp311-cp311-macosx_10_9_x86_64.macosx_11_0_arm64.macosx_10_9_universal2.whl (6.2 MB)\n",
      "Installing collected packages: tantivy\n",
      "Successfully installed tantivy-0.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tantivy # for lancedb full text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "\n",
    "import os\n",
    "import lancedb\n",
    "from lancedb.embeddings import get_registry\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '<add your OPENAI API>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text):\n",
    "    client = OpenAI()\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return client.embeddings.create(input=[text], model=\"text-embedding-3-large\").data[0].embedding\n",
    "\n",
    "def text_sanitize(text):\n",
    "    if not text:\n",
    "        return \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all\"\n",
    "#\"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\" # llmsherpa api if not self-hosted\n",
    "\n",
    "pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc = pdf_reader.read_pdf(pdf_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for chunk in doc.chunks():\n",
    "    tmp_dict = {}\n",
    "    chunk_text = chunk.to_text()\n",
    "    chunk_text = text_sanitize(chunk_text)\n",
    "    tmp_dict[\"text\"] = chunk_text\n",
    "    tmp_dict[\"context\"]= chunk.to_context_text()\n",
    "    tmp_dict[\"vector\"] = get_embedding(chunk_text)\n",
    "    data.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = lancedb.connect(\"./db\")\n",
    "table = db.create_table(\"chunks\", data=data, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.create_fts_index(\"text\")  # Create a fts index before the hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>context</th>\n",
       "      <th>vector</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Table 1: Comparison of pre-training objectives...</td>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>[0.026339235, 0.009338064, -0.02416826, 0.0143...</td>\n",
       "      <td>12.379797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Token masking is crucial Pre-training objectiv...</td>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>[-0.024983516, -0.032836277, -0.010570487, 0.0...</td>\n",
       "      <td>7.768296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Left-to-right pre-training improves generation...</td>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>[-0.014563556, -0.008050171, -0.013285295, 0.0...</td>\n",
       "      <td>7.281164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>While many pre-training objectives have been p...</td>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>[-0.0027317216, 0.017693207, -0.01675744, 0.01...</td>\n",
       "      <td>7.237228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Performance of pre-training methods varies sig...</td>\n",
       "      <td>BART: Denoising Sequence-to-Sequence Pre-train...</td>\n",
       "      <td>[-0.0013883491, 0.0031317624, -0.018050993, -0...</td>\n",
       "      <td>6.996423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Table 1: Comparison of pre-training objectives...   \n",
       "1  Token masking is crucial Pre-training objectiv...   \n",
       "2  Left-to-right pre-training improves generation...   \n",
       "3  While many pre-training objectives have been p...   \n",
       "4  Performance of pre-training methods varies sig...   \n",
       "\n",
       "                                             context  \\\n",
       "0  BART: Denoising Sequence-to-Sequence Pre-train...   \n",
       "1  BART: Denoising Sequence-to-Sequence Pre-train...   \n",
       "2  BART: Denoising Sequence-to-Sequence Pre-train...   \n",
       "3  BART: Denoising Sequence-to-Sequence Pre-train...   \n",
       "4  BART: Denoising Sequence-to-Sequence Pre-train...   \n",
       "\n",
       "                                              vector      score  \n",
       "0  [0.026339235, 0.009338064, -0.02416826, 0.0143...  12.379797  \n",
       "1  [-0.024983516, -0.032836277, -0.010570487, 0.0...   7.768296  \n",
       "2  [-0.014563556, -0.008050171, -0.013285295, 0.0...   7.281164  \n",
       "3  [-0.0027317216, 0.017693207, -0.01675744, 0.01...   7.237228  \n",
       "4  [-0.0013883491, 0.0031317624, -0.018050993, -0...   6.996423  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.search(\n",
    "    \"What are the objectives of pre-training?\"\n",
    ").limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_with_context(query, context):\n",
    "    limit = 3750\n",
    "\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(context)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(context.text[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context.text[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(context)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context.text) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(prompt):\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    message = response.choices[0].message.content\n",
    "    #print(response)\n",
    "\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pre-training, particularly in the context of machine learning and artificial intelligence, serves several important objectives that generally aim to improve the performance, adaptability, and efficiency of models. The objectives of pre-training include:\\n\\n1. **Learning General Features:** Pre-training allows models to learn general features and patterns from large datasets. This is especially useful in domains where labeled data is scarce, but unlabeled data is abundant. By pre-training on a large, unlabeled dataset, the model can learn a good representation of the input space which can be useful across a variety of tasks.\\n\\n2. **Transfer Learning:** Pre-trained models can be fine-tuned on specific, smaller tasks. This approach, known as transfer learning, is beneficial when the target task has limited training data. The pre-trained model, having learned a broad representation, needs only minimal adjustment to adapt to the specifics of the new task.\\n\\n3. **Improving Performance:** Models pre-trained on large datasets generally exhibit better performance on specific tasks compared to models trained from scratch on the same tasks. This is because pre-training helps in avoiding overfitting during the subsequent training on smaller datasets.\\n\\n4. **Reducing Training Time:** Pre-training can significantly reduce the amount of time required to train a model for a specific task. Since the model has already learned certain general features during pre-training, it only needs to adjust its weights slightly when fine-tuned on a specific task, as opposed to learning all features from scratch.\\n\\n5. **Handling Data Sparsity and Imbalance:** In cases where some classes are underrepresented, pre-trained models can help by providing a more robust initial representation that captures more general features not overly biased towards the overrepresented classes.\\n\\n6. **Multitasking Capability:** A robust pre-trained model often possesses the flexibility to be adapted to multiple different tasks without major changes in architecture or training from scratch. This is particularly useful in applications where deploying separate models for each task would be impractical.\\n\\n7. **Cross-domain Adaptability:** Pre-trained models can be adapted not just to different tasks but also to different domains (e.g., from natural images to medical images), leveraging commonalities in data representation learned during pre-training.\\n\\n8. **Cost Efficiency:** In contexts where computational resources are expensive, pre-training on a computational cluster followed by fine-tuning on smaller scale equipment can be more cost-effective.\\n\\n9. **Enhancing Model Robustness:** Pre-training can also help in enhancing the model's robustness to noise and input variations, as exposure to large and diverse datasets during pre-training helps the model generalize better.\\n\\n10. **Benchmarking and Model Evaluation:** Pre-trained models serve as benchmarks for evaluating new models or methods. They help set performance baselines that other models must meet or exceed.\\n\\nIn summary, the objectives of pre-training revolve around leveraging extensive pre-existing datasets to build robust, versatile, and efficient models that can be applied, adapted, and fine-tuned to a wide range of specific tasks and domains. This methodology underpins much of the success seen in fields such as natural language processing, computer vision, and beyond.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the objectives of pre-training?\"\n",
    "complete(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
